{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to use the MDP class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91e076dc0b7b167c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook briefly describes how to use the MDP class that is provided with the code.\n",
    "\n",
    "**The notebook format is used to give short examples, do not implement the algorithms inside a Jupyter notebook**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "815c82dea995fc30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from assignment2_mdp.mdp.reward_function import RewardFunction\n",
    "from assignment2_mdp.mdp.transition_function import TransitionFunction\n",
    "from assignment2_mdp.mdp.mdp import MDP"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T00:21:18.239277Z",
     "start_time": "2024-09-10T00:21:18.236483Z"
    }
   },
   "id": "initial_id",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall that environments can be modelled as a Markov Decision Process (MDP), which is defines as a tuple $(\\mathcal{S}, \\mathcal{A}, p, r, \\gamma)$ where\n",
    "* $\\mathcal{S}$ is the set of states.\n",
    "* $\\mathcal{A}$ is the set of actions.\n",
    "* $p : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$  is a transition function.\n",
    "* $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is a reward function.\n",
    "* $\\gamma \\in [0,1]$ is a discount factor.\n",
    "\n",
    "The ```mdp``` class can be used to implement a simple mdp with a discrete state and action space.\n",
    "\n",
    "For example, suppose we wanted to implement a simple mdp with 3 states and 2 actions:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a5a9d3f56c54a35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "states = [0, 1, 2]    # Set of states actions represented as a list of integers.\n",
    "actions = [0, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T00:21:19.600125Z",
     "start_time": "2024-09-10T00:21:19.597900Z"
    }
   },
   "id": "873c0b3cdf2cdd71",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we must specify the reward function, which should give a reward for every state action pair"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76894ffad03f9f2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Define rewards using a dictionary\n",
    "rewards = {\n",
    "    (0, 0): -1.0,           # state 0, action 0 gets reward -1.\n",
    "    (0, 1): -1.0,\n",
    "    (1, 0): 5.0,\n",
    "    (1, 1): -1.0,\n",
    "    (2, 0): -1.0,\n",
    "    (2, 1): 10.0\n",
    "}\n",
    "\n",
    "# Create the RewardFunction object\n",
    "reward_function = RewardFunction(rewards)\n",
    "\n",
    "# Now calling the reweard function\n",
    "print(reward_function(2, 1))           # This should return 10.0."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T00:21:20.969633Z",
     "start_time": "2024-09-10T00:21:20.966617Z"
    }
   },
   "id": "1c927da771efe570",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next lets specify the transition function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b62f295faa25e84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.8 0.1]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# Define transition probabilities using a dictionary\n",
    "transitions = {\n",
    "    (0, 0): np.array([0.7, 0.2, 0.1]),      # For state one, action one we get probability vector (0.7, 0.2, 0.1) representing the probability to transition to state 0, 1, 2 respectively.\n",
    "    (0, 1): np.array([0.1, 0.8, 0.1]),\n",
    "    (1, 0): np.array([0.4, 0.5, 0.1]),\n",
    "    (1, 1): np.array([0.3, 0.3, 0.4]),\n",
    "    (2, 0): np.array([0.9, 0.05, 0.05]),\n",
    "    (2, 1): np.array([0.2, 0.2, 0.6])\n",
    "}\n",
    "\n",
    "# Create the TransitionFunction object\n",
    "transition_function = TransitionFunction(transitions)\n",
    "\n",
    "# Example usage: Get the full transition probabilities array for state 0 and action 1\n",
    "print(transition_function(0, 1))  # Output: [0.1 0.8 0.1]\n",
    "print(transition_function(0, 1)[2])  # Probability P(S'=2|S=0, A=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T00:21:22.459846Z",
     "start_time": "2024-09-10T00:21:22.456255Z"
    }
   },
   "id": "38be099279bd79e2",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all you need to do is pass each component to the MDP class:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b6e2fcd93ad420b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the MDP object\n",
    "mdp = MDP(states, actions, transition_function, reward_function, discount_factor=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T00:21:24.625108Z",
     "start_time": "2024-09-10T00:21:24.622876Z"
    }
   },
   "id": "268f72a22840e332",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability from state 0 to state 1 with action 1: 0.8\n",
      "Reward for taking action 1 in state 2: 10.0\n",
      "States: [0, 1, 2]\n",
      "Actions: [0, 1]\n",
      "Discount Factor: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Example usage of MDP functionalities:\n",
    "\n",
    "# Get transition probability from state 0 to state 1 with action 1\n",
    "print(f\"Transition probability from state 0 to state 1 with action 1: {mdp.transition_prob(0, 1, 1)}\")\n",
    "\n",
    "# Get reward for taking action 1 in state 2\n",
    "print(f\"Reward for taking action 1 in state 2: {mdp.reward(2, 1)}\")\n",
    "\n",
    "# Get list of states\n",
    "print(f\"States: {mdp.states}\")\n",
    "\n",
    "# Get list of actions\n",
    "print(f\"Actions: {mdp.actions}\")\n",
    "\n",
    "# Get discount factor\n",
    "print(f\"Discount Factor: {mdp.discount_factor}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T00:21:25.889985Z",
     "start_time": "2024-09-10T00:21:25.887326Z"
    }
   },
   "id": "4204bde02869c883",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the assignment you will need to implement the `BellmanEquationSolver` and `DynamicProgrammingSolver` which should take a MDP as argument to their constructors and offer\n",
    "methods that return a policy, which in this case can simply be a array (or dictionary) that maps states to actions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "330fd4324bf9da49"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
